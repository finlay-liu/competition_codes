# -*- coding: utf-8 -*-
"""天池nlp预训练泛化挑战.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/
"""

#!nvidia-smi

import pandas as pd
import numpy as np
import os, sys
from google.colab import drive

'''如果在google colab上运行需要执行以下代码'''
drive.mount('/content/drive')
nb_path = '/content/drive/My Drive/Colab Notebooks/'
sys.path.insert(0,nb_path)

'''
路径说明：
../code #保存代码
../data #保存数据
../subs #保存数据
../chinese_roberta_wwm_large_ext_L-24_H-1024_A-16 #bert路径
'''

#一些调优后的参数
er_patience = 2 #early_stopping patience
lr_patience = 5 #ReduceLROnPlateau patience
max_epochs = 10 #epochs
lr_rate = 3e-6 #learning rate
batch_sz = 4 #batch_size
maxlen = 256  #设置序列长度为，base模型要保证序列长度不超过512
lr_factor = 0.85 #ReduceLROnPlateau factor
maxlentext1 = 200
n_folds = 10 

path = "/content/drive/My Drive/天池nlp预训练/"

#path_bert = path + '/chinese_rbt6_L-6_H-768_A-12/'
path_bert = path + '/chinese_roberta_wwm_large_ext_L-24_H-1024_A-16/'

#pip install keras-bert #keras tensorflow pycaret fasttext

import pandas as pd
import codecs, gc
import numpy as np
from sklearn.model_selection import KFold
from keras_bert import load_trained_model_from_checkpoint, Tokenizer
from keras.metrics import top_k_categorical_accuracy
from keras.layers import *
from keras.callbacks import *
from keras.models import Model
import keras.backend as K
from keras.optimizers import Adam
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder

#将ocnli中content1[0:maxlentext1]+content2作为ocnli的content输入
times_train = pd.read_csv(path + '/data/TNEWS_train1128.csv',  sep='\t', header=None, names=('id', 'content', 'label')).astype(str)
ocemo_train = pd.read_csv(path + '/data/OCEMOTION_train1128.csv',sep='\t', header=None, names=('id', 'content', 'label')).astype(str)
ocnli_train = pd.read_csv(path + '/data/OCNLI_train1128.csv',  sep='\t', header=None, names=('id', 'content1', 'content2', 'label')).astype(str)
ocnli_train['content'] = ocnli_train['content1'].apply( lambda x: x[:maxlentext1] ) + ocnli_train['content2']

times_testa = pd.read_csv(path + '/data/TNEWS_a.csv',  sep='\t', header=None, names=('id', 'content')).astype(str)
ocemo_testa = pd.read_csv(path + '/data/OCEMOTION_a.csv',sep='\t', header=None, names=('id', 'content')).astype(str)
ocnli_testa = pd.read_csv(path + '/data/OCNLI_a.csv',  sep='\t', header=None, names=('id', 'content1', 'content2')).astype(str)
ocnli_testa['content'] = ocnli_testa['content1'].apply( lambda x: x[:maxlentext1] ) + ocnli_testa['content2']

#构造输入、标签数据
#合并三个任务的训练、测试数据
train_df = pd.concat([times_train, ocemo_train, ocnli_train[['id','content', 'label']]], axis=0).copy()
testa_df = pd.concat([times_testa, ocemo_testa, ocnli_testa[['id', 'content']]], axis=0).copy()

#LabelEncoder处理标签，因为label需要从0开始
encode_label = LabelEncoder()
train_df['label'] = encode_label.fit_transform(train_df['label'].apply(str))

###采用分层抽样的方式，从训练集中抽取10%作为验证机
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=222)

X_trn = pd.DataFrame()
X_val = pd.DataFrame()

for train_index, test_index in skf.split(train_df.copy(), train_df['label']):
  X_trn, X_val = train_df.iloc[train_index], train_df.iloc[test_index]
  break
###

subs = testa_df[['id']]
testa_df = testa_df[['content']]

#训练数据、测试数据和标签转化为模型输入格式
n_cls = len( train_df['label'].unique() )

#训练集每行的content、label转为tuple存入list，再转为numpy array
TRN_LIST = []
for data_row in X_trn.iloc[:].itertuples():
  TRN_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))
TRN_LIST = np.array(TRN_LIST)

#验证集每行的content、label转为tuple存入list，再转为numpy array
VAL_LIST = []
for data_row in X_val.iloc[:].itertuples():
  VAL_LIST.append((data_row.content, to_categorical(data_row.label, n_cls)))
VAL_LIST = np.array(VAL_LIST)

#测试集每行的content、label转为tuple存入list，再转为numpy array，其中label全为0
DATA_LIST_TEST = []
for data_row in testa_df.iloc[:].itertuples():
  DATA_LIST_TEST.append((data_row.content, to_categorical(0, n_cls)))
DATA_LIST_TEST = np.array(DATA_LIST_TEST)

#预训练好的模型
config_path = path_bert + 'bert_config.json'
dict_path  = path_bert + 'vocab.txt'
checkpoint_path = path_bert + 'bert_model.ckpt'

#将词表中的词编号转换为字典
token_dict = {}
with codecs.open(dict_path, 'r', 'utf8') as reader:
  for line in reader:
    token = line.strip()
    token_dict[token] = len(token_dict)

#重写tokenizer        
class OurTokenizer(Tokenizer):
  def _tokenize(self, text):
    R = []
    for c in text:
      if c in self._token_dict:
        R.append(c)
      elif self._is_space(c):
        R.append('[unused1]') # 用[unused1]来表示空格类字符
      else:
        R.append('[UNK]')   # 不在列表的字符用[UNK]表示
    return R

tokenizer = OurTokenizer(token_dict)
 
#让每条文本的长度相同，用0填充
def seq_padding(X, padding=0):
  L = [len(x) for x in X]
  ML = max(L)
  return np.array([  np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X  ])
  
#data_generator只是一种为了节约内存的数据方式
class data_generator:
  global batch_sz
  def __init__(self, data, batch_size=batch_sz, shuffle=True):#此处修改batch_size
    self.data = data
    self.batch_size = batch_size
    self.shuffle = shuffle
    self.steps = len(self.data) // self.batch_size
    if len(self.data) % self.batch_size != 0:
      self.steps += 1

  def __len__(self):
    return self.steps

  def __iter__(self):
    while True:
      idxs = list(range(len(self.data)))
      if self.shuffle:
        np.random.shuffle(idxs)
      X1, X2, Y = [], [], []
      for i in idxs:
        d = self.data[i]
        text = d[0][: maxlen]
        #text2 = d[0][maxlentext1:maxlen]
        x1, x2 = tokenizer.encode(first=text)#1, second=text2)#修改成输入两个子句

        y = d[1]
        X1.append(x1)
        X2.append(x2)
        Y.append([y])
        if len(X1) == self.batch_size or i == idxs[-1]:
          X1 = seq_padding(X1)
          X2 = seq_padding(X2)
          Y = seq_padding(Y)
          yield [X1, X2], Y[:, 0, :]
          [X1, X2, Y] = [], [], []

#计算top-k正确率,当预测值的前k个值中存在目标类别即认为预测正确                 
def acc_top2(y_true, y_pred):
  return top_k_categorical_accuracy(y_true, y_pred, k=2)

def boolMap(arr):
  if arr > 0.5:
      return 1
  else:
      return 0

#自定义计算每个epoch的F1，注意不是batch的F1
class Metrics(Callback):
  def __init__(self, filepath):
    self.file_path = filepath

  def on_train_begin(self, logs=None):
    self.val_f1s = []
    self.best_val_f1 = 0
    self.val_recalls = []
    self.val_precisions = []

  def on_epoch_end(self, epoch, logs=None):
    val_predict = list(map(boolMap, self.model.predict([self.validation_data[0], self.validation_data[1]])))
    val_targ = self.validation_data[2]
    _val_f1 = f1_score(val_targ, val_predict)
    _val_recall = recall_score(val_targ, val_predict)
    _val_precision = precision_score(val_targ, val_predict)
    self.val_f1s.append(_val_f1)
    self.val_recalls.append(_val_recall)
    self.val_precisions.append(_val_precision)
    print(_val_f1, _val_precision, _val_recall)
    print("max f1")
    print(max(self.val_f1s))
    if _val_f1 > self.best_val_f1:
      self.model.save_weights(self.file_path, overwrite=True)
      self.best_val_f1 = _val_f1
      print("best f1: {}".format(self.best_val_f1))
    else:
      print("val f1: {}, but not the best f1".format(_val_f1))
    return

f1metrics = Metrics(path)

#计算每个batch的f1
from keras import backend as K

def f1(y_true, y_pred):
  def recall(y_true, y_pred):
    """Recall metric.

    Only computes a batch-wise average of recall.

    Computes the recall, a metric for multi-label classification of
    how many relevant items are selected.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

  def precision(y_true, y_pred):
    """Precision metric.

    Only computes a batch-wise average of precision.

    Computes the precision, a metric for multi-label classification of
    how many selected items are relevant.
    """
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision
  precision = precision(y_true, y_pred)
  recall = recall(y_true, y_pred)
  return 2*((precision*recall)/(precision+recall+K.epsilon()))

#bert模型设置
def build_bert(nclass):
  global lr_rate
  bert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)  #加载预训练模型

  for l in bert_model.layers:
    l.trainable = True

  x1_in = Input(shape=(None,))
  x2_in = Input(shape=(None,))

  x = bert_model([x1_in, x2_in])
  x = Lambda(lambda x: x[:, 0])(x) #取出[CLS]对应的向量用来做分类
  p = Dense(nclass, activation='softmax')(x) #直接dense层softmax输出

  model = Model([x1_in, x2_in], p)
  model.compile(loss='categorical_crossentropy',
                optimizer=Adam(lr_rate),    #用足够小的学习率
                metrics=['accuracy', f1])#acc_top2

  print(model.summary())
  return model

#交叉验证训练和测试模型
def run_nocv(nfold, trn_data, val_data, data_labels, data_test, n_cls):
  global er_patience
  global lr_patience
  global max_epochs
  global f1metrics
  global lr_factor
  test_model_pred = np.zeros((len(data_test), n_cls))

  model = build_bert(n_cls)
  #下行代码用于加载保存的权重继续训练
  #model.load_weights(path + '/subs/model.epoch01_val_loss1.0085_val_acc0.6360_val_f10.6170.hdf5')
  
  early_stopping = EarlyStopping(monitor= "val_f1", patience=er_patience) #早停法，防止过拟合 #'val_accuracy'
  plateau = ReduceLROnPlateau(monitor="val_f1", verbose=1, mode='max', factor=lr_factor, patience=lr_patience) #当评价指标不在提升时，减少学习率 #"val_accuracy"
  checkpoint = ModelCheckpoint(path + "/subs/model.epoch{epoch:02d}_val_loss{val_loss:.4f}_val_acc{val_accuracy:.4f}_val_f1{val_f1:.4f}.hdf5", monitor="val_f1", verbose=2, save_best_only=True, mode='max', save_weights_only=True) #保存val_f1最好的模型权重#'val_acc'

  #训练跟验证集可shuffle打乱，测试集不可打乱（否则在生成结果文件的时候没法跟ID对应上）
  train_D = data_generator(trn_data, shuffle=True)
  valid_D = data_generator(val_data, shuffle=True)
  test_D = data_generator(data_test, shuffle=False)
  
  #模型训练
  model.fit_generator(
      train_D.__iter__(),
      steps_per_epoch=len(train_D),
      epochs=max_epochs,
      validation_data=valid_D.__iter__(),
      validation_steps=len(valid_D),
      callbacks=[early_stopping, plateau, checkpoint],
  )
  #模型预测
  test_model_pred = model.predict_generator(test_D.__iter__(), steps=len(test_D), verbose=1)
  train_model_pred = model.predict(train_D.__iter__(), steps=len(train_D), verbose=1)
 
  del model
  gc.collect()   #清理内存
  K.clear_session() #clear_session就是清除一个session

  return test_model_pred, train_model_pred

"""
#此cell代码为训练代码
cvs = 1
#输出为numpy array格式的25列概率
test_model_pred, train_model_pred = run_nocv(cvs, TRN_LIST, VAL_LIST, None, DATA_LIST_TEST, n_cls)

#将结果转为DataFrame格式
preds_tst_df = pd.DataFrame(test_model_pred)
preds_trn_df = pd.DataFrame(train_model_pred)

#再将range(0,25)做encode_label逆变换作为该DataFrame的列名
preds_col_names = encode_label.inverse_transform( range(0,n_cls) )
preds_tst_df.columns = preds_col_names
preds_trn_df.columns = preds_col_names

#保存输出概率，将该输出概率输入树模型还能提升分数，大概4个千分点
preds_tst_df.to_csv(path + "/subs/tstsprob.csv", index=None)
preds_trn_df.to_csv(path + "/subs/trnsprob.csv", index=None)

"""

#以下代码为加载模型权重直接预测
#用于复现跟赛题第二阶段
model_name = "/subs/model.epoch03_val_loss_val_acc_val_f1.hdf5"#模型权重路径跟名称
test_prob_name = "/subs/prob_xxxx.csv"

#rebuild model as model2
#simulating starting a new script
model2 = build_bert(25)

#load weights
model2.load_weights(path + model_name)

test_D  = data_generator(DATA_LIST_TEST, shuffle=False)
test_model_pred = model2.predict_generator(test_D.__iter__(), steps=len(test_D), verbose=1)

preds_tst_df = pd.DataFrame(test_model_pred)

preds_col_names = encode_label.inverse_transform( range(0,n_cls) )
preds_tst_df.columns = preds_col_names

preds_tst_df.to_csv(path + test_prob_name, index=None)

#以上代码为加载模型权重直接预测
#times_testa, ocemo_testa, ocnli_testa
#每行，从每个任务对应的标签列中找出最大的概率对应的列名作为预测结果
#如ocnli任务的预测结果只能为0、1、2，那么从preds_tst_df中选择0-1-2三列中每行概率最大的列名作为ocnli任务的测试集预测结果
#如ocemotion任务的预测结果只能为sadness、like、fear等7类，那么从preds_tst_df中选择sadness、like、fear等7列中每行概率最大的列名作为ocnli任务的测试集预测结果

times_preds = preds_tst_df.head(times_testa.shape[0])[times_train['label'].unique().tolist()]
times_preds = times_preds.eq(times_preds.max(1), axis=0).dot(times_preds.columns)

ocemo_preds = preds_tst_df.head(times_testa.shape[0] + ocemo_testa.shape[0]).tail(ocemo_testa.shape[0])[ocemo_train['label'].unique().tolist()]
ocemo_preds = ocemo_preds.eq(ocemo_preds.max(1), axis=0).dot(ocemo_preds.columns)

ocnli_preds = preds_tst_df.tail(ocnli_testa.shape[0])[ocnli_train['label'].unique().tolist()]
ocnli_preds = ocnli_preds.eq(ocnli_preds.max(1), axis=0).dot(ocnli_preds.columns)

times_sub = times_testa[['id']].copy()
times_sub['label'] = times_preds.values
times_sub.to_json(path + "/subs/tnews_predict.json", orient='records', lines=True)

ocemo_sub = ocemo_testa[['id']].copy()
ocemo_sub['label'] = ocemo_preds.values
ocemo_sub.to_json(path + "/subs/ocemotion_predict.json", orient='records', lines=True)

ocnli_sub = ocnli_testa[['id']].copy()
ocnli_sub['label'] = ocnli_preds.values
ocnli_sub.to_json(path + "/subs/ocnli_predict.json", orient='records', lines=True)

print(times_testa.shape, ocemo_testa.shape, ocnli_testa.shape)